import os
import sys

# âœ… å…ˆæŠŠé¡¹ç›®æ ¹ç›®å½•åŠ è¿› sys.pathï¼Œå† import deep_research ç­‰å†…éƒ¨åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# æ ‡å‡†åº“ import
import asyncio
import wave
import json
import base64
import threading
import queue
from typing import Optional, Callable
from datetime import datetime
from http import HTTPStatus
import numpy as np

try:
    import pyaudio
except ImportError:
    print(f'âŒpyaudio not installed. Installing...')
    os.system('pip install pyaudio')
    import pyaudio

from openai import OpenAI
import dashscope
from dashscope.audio.asr import Recognition, RecognitionCallback

# Deep research agent importsï¼ˆç°åœ¨ sys.path å·²ç»è®¾ç½®å¥½ï¼Œå¯ä»¥å®‰å…¨ importï¼‰
from langchain_openai import ChatOpenAI
from langchain_community.utilities import SearxSearchWrapper
from deep_research.service import DeepResearchService

class VoiceAIConfig:
	def __init__(self, mode:str="traditional", search_type:str="rag"):
		self.api_key=os.environ.get('DASHSCOPE_API_KEY')
		if not self.api_key:
			raise ValueError("DASHSCOPE_API_KEY environment variable is not set")

		self.mode = mode
		self.search_type = search_type

		self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

		if mode == 'traditional':
			self.asr_model = 'fun-asr-realtime-2025-11-07'
			self.llm_model = 'deepseek-v3.2'
			self.tts_model = 'sambert-zhichu-v1'
			self.tts_voice = 'zhichu'
		else:
			# self.omni_model = 'qwen-omni-turbo'
			self.omni_model = 'qwen3-omni-flash-2025-12-01'

		# audio configuration 
		self.sample_rate=16000
		self.channels = 1
		self.chunk_size = 3200
		self.format = pyaudio.paInt16

		# VAD Voice Activity Detection
		self.vad_threshold = 500
		self.silence_duration = 2.0

		# LLM configuration 
		self.enable_thinking = False
		self.temperature = 0.7
		self.max_tokens = 2000

		# RAG configuration
		self.enable_rag = False
		self.system_prompt = (
			"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ä»·/è®®ä»·è¯­éŸ³åŠ©æ‰‹ï¼Œç›®æ ‡æ˜¯åœ¨ä¸å¤±ç¤¼çš„å‰æä¸‹ï¼Œä¸ºç”¨æˆ·äº‰å–æ›´ä½ä»·æ ¼æˆ–æ›´å¥½æƒç›Šã€‚\n"
			"ä½ ä¼šæ”¶åˆ°ä¸¤ç±»ä¿¡æ¯ï¼š\n"
			"1) å•†å®¶åˆšåˆšè¯´çš„è¯ï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰\n"
			"2) èƒŒæ™¯çŸ¥è¯†ï¼ˆRAG æ£€ç´¢åˆ°çš„ï¼šå‚è€ƒä»·æ ¼ã€å¸¸è§ä¼˜æƒ ã€ç«å“ä»·æ ¼ã€æ´»åŠ¨è§„å¾‹ã€é£é™©ç‚¹ç­‰ï¼‰\n\n"
			"ä½ çš„è¾“å‡ºå¿…é¡»æ»¡è¶³ï¼š\n"
			"- åªè¾“å‡ºç”¨æˆ·ä¸‹ä¸€å¥è¦å¯¹å•†å®¶è¯´çš„è¯ï¼ˆå£è¯­åŒ–ã€ç®€çŸ­ã€æœ‰ç¤¼è²Œï¼‰ã€‚\n"
			"- æ˜ç¡®ç›®æ ‡ï¼šé™ä»· / èµ å“ / åŒ…é‚® / å»¶ä¿ / ä»¥æ—§æ¢æ–° / å‘ç¥¨ / åˆ†æœŸä¼˜æƒ  ç­‰ï¼Œæ‹©ä¸€ä¸¤é¡¹é‡ç‚¹æ¨è¿›ã€‚\n"
			"- ä¸è¦ç¼–é€ äº‹å®ï¼šå¦‚æœèƒŒæ™¯çŸ¥è¯†æ²¡æœ‰æ˜ç¡®æ•°æ®ï¼Œä¸è¦æŠ¥å…·ä½“æ•°å­—ï¼Œç”¨â€œæˆ‘çœ‹åˆ°ç±»ä¼¼ä»·æ ¼/æœ‰è¿‡æ´»åŠ¨â€ç­‰æ¨¡ç³Šè¡¨è¾¾ã€‚\n"
			"- ä¼˜å…ˆç”¨å¯æ‰§è¡Œçš„è°ˆåˆ¤ç­–ç•¥ï¼šé”šå®šã€å¯¹æ¯”ã€è®©æ­¥æ¢å–ã€é™æ—¶å†³ç­–ã€ç»„åˆè¯‰æ±‚ï¼ˆé™ä»·+èµ å“ï¼‰ã€è¯·æ±‚ä¸Šçº§å®¡æ‰¹ã€‚\n"
			"- å¦‚æœå•†å®¶æ‹’ç»ï¼Œç»™å‡ºä¸‹ä¸€æ­¥å¤‡é€‰æ–¹æ¡ˆï¼ˆä¾‹å¦‚è½¬èµ å“ã€åŒ…é‚®ã€è¿”ç°ã€å»¶ä¿ï¼‰ã€‚\n"
		)
		# âœ… åˆ é™¤è¿™è¡Œï¼Œå®ƒè¦†ç›–äº†ä¸Šé¢çš„ç ä»· prompt
		# self.system_prompt = 'Please respond shortly without expressive words'
		self.rag_context = ''
		
		# Deep research agent configuration
		# Get API keys for deep research agent
		self.ali_api_key = os.environ.get('ALI_API_KEY') or self.api_key  # Fallback to DASHSCOPE_API_KEY
		self.searx_host = os.environ.get('SEARX_HOST', 'http://127.0.0.1:38000')
class MicrophoneInput:
	def __init__(self, config: VoiceAIConfig):
		self.config = config
		self.audio = pyaudio.PyAudio()
		self.stream = None # please find the stream usage 
		self.is_recording = False
		self.is_listening = False
		self.audio_queue = queue.Queue()

	def start(self):
		# start microphone input stream (but not capturing)
		try:
			self.stream = self.audio.open(
				format=self.config.format,
				channels=self.config.channels,
				rate=self.config.sample_rate,
				input=True,
				frames_per_buffer=self.config.chunk_size,
				stream_callback=self._audio_callback)
			self.stream.start_stream()
			print(f'ğŸ™ï¸ Microphone ready')
		except Exception as e:
			print(f'âŒfail to start microphone:{e}')
			raise

	def enable_listening(self):
		# enabling audio capture - only capture audio when this is called
		while not self.audio_queue.empty():
			try:
				self.audio_queue.get_nowait()
			except queue.Empty:
				break
		self.is_listening = True
		self.is_recording = True
		print (f'ğŸ™ï¸ Microphone Enabled - Listening your voice...')

	def disable_listening(self):
		#disabling audio capture - prevent capturing AI output 
		self.is_listening = False
		self.is_recording = False
		while not self.audio_queue.empty():
			try:
				self.audio_queue.get_nowait()
			except queue.Empty:
				break
		
		print (f'ğŸ™ï¸ Microphone Disabled - not capture AI voice')

	def _audio_callback(self, in_data, frame_count, time_info, status):
		# Callback for audio stream - only capture when listening is enabled
		if self.is_recording and self.is_listening:
			self.audio_queue.put(in_data)
		return (in_data, pyaudio.paContinue)

	def read_chunk(self) -> Optional[bytes]:
		# read one chunk for audio data
		# !!! there is one problem that one chunk is too big 
		try:
			return self.audio_queue.get(timeout=0.1)
		except queue.Empty:
			return None

	def stop(self):
		# stop microphone input completely 
		self.is_listening = False
		self.is_recording = False
		if self.stream:
			self.stream.stop_stream()
			self.stream.close()
		self.audio.terminate()
		print (f'ğŸ™ï¸ Microphone terminated')

class SpeakerOutput: 
	def __init__(self, config:VoiceAIConfig):
		self.config = config
		self.audio = pyaudio.PyAudio()
		self.stream = None # please find the stream usage 
		self.is_playing = False

	def start(self):
		'''start speaker output stream'''
		try:
			self.stream = self.audio.open(
				format=self.config.format,
				channels=self.config.channels,
				rate=self.config.sample_rate,
				output=True,
				frames_per_buffer=self.config.chunk_size)
			print(f'ğŸ“¢ speaker is ready')
		except Exception as e: 
			print(f'âŒfail to start speaker:{e}')
			raise

	def play(self, audio_data: bytes):
		'''Play audio data through speaker'''
		if self.stream and not self.is_playing:
			self.is_playing = True
			self.stream.write(audio_data)
			self.is_playing = False

	def stop(self):
		'''stop speaker output'''
		if self.stream:
			self.stream.stop_stream()
			self.stream.close()
		self.audio.terminate()
		print (f'ğŸ“¢ speaker terminated')

class RealTimeVoiceAI:
	'''
	voice in - ASR - LLM - tts - voice out
	'''
	def __init__(self, config: VoiceAIConfig):
		self.config = config
		self.microphone = MicrophoneInput(config)
		self.speaker = SpeakerOutput(config)

		dashscope.api_key = config.api_key

		self.llm_client = OpenAI(
			api_key = config.api_key,
			base_url = config.base_url
			)

		self.conversation_history = []

		self.is_running = False
		self.is_listening = False
		
		# Initialize deep research service
		print(f"ğŸ”§ Initializing deep research service...")
		print(f"  SearxNG host: {config.searx_host}")
		# LLM for deep research agent
		deep_research_llm = ChatOpenAI(
			model='deepseek-r1',
			api_key=config.ali_api_key,
			base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
		)
		print(f"  Deep research LLM: deepseek-r1")
		# Search engine for deep research agent
		search_engine = SearxSearchWrapper(searx_host=config.searx_host, k=3)
		print(f"  Search engine: SearxSearchWrapper (k=3)")
		# Create deep research service instance
		self.deep_research_service = DeepResearchService(deep_research_llm, search_engine)
		print(f"âœ… Deep research service initialized")

	def set_rag_context(self, context: str):
		# set knowledge base for injecting 
		self.config.rag_context = context
		self.config.enable_rag = True if context else False
		print(f'rag context is updated: {len(context)} characters')

	def set_system_prompt(self, prompt: str):
		self.config.system_prompt = prompt
		print(f'rag context is updated: {prompt[:50]} ...')

	def clear_chat_history(self):
		self.conversation_history = []
		print(f'chat history is clear')

	def initialize(self):
		# start audio IO
		self.microphone.start()
		self.speaker.start()

	def transcribe_audio(self, audio_data: bytes) -> str:
		# audio -> text
		try: 
			# save audio to temporary file
			# audio saving for how long 
			print(f'Transcribing {len(audio_data)} bytes of audio')
			temp_file = '/tmp/voice_ai_input.wav'
			with wave.open(temp_file, 'wb') as wf:
				wf.setnchannels(self.config.channels)
				wf.setsampwidth(self.microphone.audio.get_sample_size(self.config.format))
				wf.setframerate(self.config.sample_rate)
				wf.writeframes(audio_data)

			recognition = Recognition(
				model=self.config.asr_model,
				format='wav',
				sample_rate=self.config.sample_rate,
				callback=None
				)

			result = recognition.call(temp_file)
			print(f'Recognition result raw: {result}')

			if result and 'output' in result:
				text = result['output']['sentence'][0]['text']
				print(f'transcription: {text}')
				return text
			else: 
				print(f'NO transcription')
				return ''

		except Exception as e:
			print(f'âŒfail to transcript:{e}')

	def get_llm_response(self, user_text: str) -> str:
		"""
		Args:
			user_text: user;s transcribed text

		Output:
			LLM response text
		"""

		if len(self.conversation_history) == 0:
			# add system prompt 
			system_context = self.config.system_prompt

			# add rag context
			if self.config.enable_rag and self.config.rag_context:
				system_context += f'\n\ncontext/knowledge base:\n{self.config.rag_context}'
				print(f'rag context is added to system prompt with {len(self.config.rag_context)} characters')

			# add user message to history
			self.conversation_history.append({
					'role':'system',
					'content':system_context
				})

		self.conversation_history.append({
			'role':'user',
			'content':user_text
		})
		print(self.conversation_history)

		try:
			response = self.llm_client.chat.completions.create(
				model=self.config.llm_model,
				messages=self.conversation_history,
				temperature=self.config.temperature,
				max_tokens=self.config.max_tokens,
				stream=False #!!!find meaning
				)

			assistant_text = response.choices[0].message.content


			self.conversation_history.append({
				'role':'assistant',
				'content':assistant_text
			})
			return assistant_text
		except Exception as e:
			print(f'âŒLLM error:{e}')
			return "sorry, I cannot answer"

	def fetch_deep_research_background(self, brand: str) -> str:
		"""
		Call deep-research-agent directly (no HTTP) to get background info and return a summary string.
		"""
		try:
			# query = (
			# 	f"æˆ‘æ­£åœ¨å’Œå•†å®¶ç ä»·ï¼Œä»¥ä¸‹æ˜¯å½“å‰å…³é”®ä¿¡æ¯/å•†å®¶è¯æœ¯ï¼š'{brand}'ã€‚\n"
			# 	"è¯·å¿«é€Ÿæ£€ç´¢å¹¶æ€»ç»“ç”¨äºç ä»·çš„èƒŒæ™¯ä¿¡æ¯ï¼Œè¦æ±‚ç®€æ´ã€å¯ç›´æ¥ç”¨äºè°ˆåˆ¤ï¼š\n"
			# 	"1) è¯¥å•†å“/æœåŠ¡çš„å¸¸è§æˆäº¤ä»·åŒºé—´ï¼ˆæœ€è¿‘/è¿‘æœŸï¼‰ä¸å½±å“å› ç´ ï¼ˆç‰ˆæœ¬ã€æ¸ é“ã€åœ°åŒºã€å¥—é¤ç­‰ï¼‰\n"
			# 	"2) ç«å“æˆ–åŒç±»æ›¿ä»£çš„ä»·æ ¼å¯¹æ¯”ï¼ˆåŒæ¡£ä½/åŒé…ç½®ï¼‰\n"
			# 	"3) å¸¸è§ä¼˜æƒ æ–¹å¼ï¼šæ»¡å‡ã€åˆ¸ã€èµ å“ã€å»¶ä¿ã€åŒ…é‚®ã€å®‰è£…ã€è¿”ç°ã€åˆ†æœŸå…æ¯ç­‰\n"
			# 	"4) å¦‚æœå•†å®¶æŠ¥ä»·åé«˜ï¼šå¯ä»¥ä½¿ç”¨çš„åˆç†ç†ç”±/è¯æœ¯ç‚¹ï¼ˆä¸è¦ç¼–é€ ï¼‰\n"
			# 	"5) é£é™©/æ³¨æ„äº‹é¡¹ï¼ˆä½ä»·é™·é˜±ã€ç¿»æ–°ã€æ¸ é“ä¿ä¿®å·®å¼‚ç­‰ï¼‰\n"
			# 	"è¾“å‡ºä¸ºè¦ç‚¹åˆ—è¡¨ï¼Œé¿å…é•¿ç¯‡å¤§è®ºã€‚"
			# )
			query=brand.strip()
			print(f"ğŸ”¬ Running deep research for brand '{brand}'...")
			print(f"ğŸ“ Query: {query[:100]}...")
			print(f"ğŸ” This will trigger search operations via SearxNG...")
			answer = self.deep_research_service.run(query, mode=self.config.search_type)
			if not answer:
				print(f"âš ï¸  Deep research returned empty answer")
				return ""
			print(f"âœ… Deep research succeeded for brand '{brand}'")
			print(f"ğŸ“Š Answer length: {len(answer)} characters")
			print(f"ğŸ“„ Answer preview: {answer[:200]}...")
			return answer
		except Exception as e:
			print(f"âŒ Deep research failed: {e}")
			import traceback
			traceback.print_exc()
			return ""

	def prime_rag_with_brand(self, brand: str):
		"""
		Run deep research for the brand and seed the RAG context for subsequent turns.
		"""
		if not brand:
			return
		research = self.fetch_deep_research_background(brand)
		if not research:
			print("Deep research returned empty; continuing without RAG context.")
			return
		self.set_rag_context(research)
		# Make the system prompt explicitly brand-aware
		self.set_system_prompt(
			f"ä½ æ˜¯ç ä»·/è®®ä»·è¯­éŸ³åŠ©æ‰‹ã€‚å½“å‰ç›®æ ‡å•†å“/å•†å®¶ä¿¡æ¯ï¼š{brand}ã€‚\n"
			"ä½ ä¼šåŸºäºåç»­æä¾›çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå¸®ç”¨æˆ·ç”¨ç¤¼è²Œä½†åšå®šçš„æ–¹å¼äº‰å–æ›´ä½ä»·æ ¼æˆ–æ›´å¥½æƒç›Šã€‚\n"
			"åªè¾“å‡ºç”¨æˆ·ä¸‹ä¸€å¥è¦è¯´çš„è¯ï¼Œç®€çŸ­å£è¯­åŒ–ã€‚"
		)

	def synthesize_speech(self, text:str) -> bytes:
		"""
		Args:
			user_text: LLM synthesis text

		Output:
			audio bytes
		"""
		try: 
			# result = dashscope.audio.tts.SpeechSynthesizer.call(
			# 	model=self.config.tts_model,
			# 	text=text,
			# 	sample_rate=self.config.sample_rate,
			# 	format='pcm')
			result = dashscope.audio.tts.SpeechSynthesizer.call(
				model=self.config.tts_model,
				text=text,
				sample_rate=self.config.sample_rate,
				format='wav'  # âœ… æ”¹æˆ wav
			)

			if result:
				try:
					audio_data = result.get_audio_data()
					if audio_data:
						print(f'synthesis {len(audio_data)} bytes audio')
						return audio_data
				except Exception as e:
					print(f'âŒaudio synthesis error:{e}')

			return b''
		except Exception as e:
			print(f'âŒTTS error:{e}')
			return b''

	def record_voice_input(self) -> bytes:
		# record until slience 
		self.microphone.enable_listening()
		frames = []
		silent_chunks = 0 
		max_silent_chunks = int(self.config.silence_duration * self.config.sample_rate / self.config.chunk_size) #!!! why equation is this
		print(f'max_silent_chunks_length{max_silent_chunks}')
		started_speaking = False

		try:
			while True:
				chunk = self.microphone.read_chunk()
				if chunk is None:
					continue 

				frames.append(chunk)

				import array
				audio_array = array.array('h', chunk) #!!! meaning
				max_amplitude = max(abs(x) for x in audio_array)

				if max_amplitude > self.config.vad_threshold:
					silent_chunks = 0
					if not started_speaking:
						started_speaking = True
						print(f'ğŸ™ï¸ Speaking detected')
				else:
					if started_speaking:
						silent_chunks += 1

				if started_speaking and silent_chunks >= max_silent_chunks:
					print(f'ğŸ™ï¸ silence detected, stop recording')
					break
		finally:
			self.microphone.disable_listening()
		
		return b''.join(frames)

	def process_voice_with_omni(self, audio_data:bytes) -> bytes:
		try:
			temp_file = '/tmp/voice_ai_omni_input.wav'
			with wave.open(temp_file, 'wb') as wf:
				wf.setnchannels(self.config.channels)
				wf.setsampwidth(self.microphone.audio.get_sample_size(self.config.format))
				wf.setframerate(self.config.sample_rate)
				wf.writeframes(audio_data)

			with open(temp_file, "rb") as audio_file:
				base64_audio = base64.b64encode(audio_file.read()).decode("utf-8")
			
			messages = []
			system_context = self.config.system_prompt
			user_context_list = []

			# The audio part
			audio_part = {
				"type": "input_audio",
				"input_audio": {
					"data": f"data:;base64,{base64_audio}",
					"format": "wav",
				},
			}
			user_context_list.append(audio_part)

			# The text part (for RAG)
			if self.config.enable_rag and self.config.rag_context:
				system_context += f'\n\ncontext/knowledge base:\n{self.config.rag_context}'
				text_part = {
					"type": "text",
					"text": "Please answer based on provided context"
				}
				user_context_list.append(text_part)

			if system_context:
				messages.append({
					'role':'system',
					'content':system_context
				})
			
			messages.append({
				'role':'user',
				'content':user_context_list
			})
			
			print(f'Sending request to Omni model: {self.config.omni_model}')
			client = OpenAI(
				api_key=self.config.api_key,  # ç¡®è®¤å·²é…ç½®ç¯å¢ƒå˜é‡
				base_url=self.config.base_url,
				)
			# responses = dashscope.MultiModalConversation.call(
			# 	model=self.config.omni_model,
			# 	messages=messages,
			# 	stream=True
			# 	)
			responses = client.chat.completions.create(
				model=self.config.omni_model,
				messages=messages,
				modalities=["text", "audio"],  # æŒ‡å®šè¾“å‡ºæ–‡æœ¬å’ŒéŸ³é¢‘
				audio={"voice": "Cherry", "format": "wav"},
				stream=True,  # å¿…é¡»è®¾ç½®ä¸º True
				stream_options={"include_usage": True},
				)
			
			audio_chunks = []
			audio_base64_string = b""
			p = pyaudio.PyAudio()
			stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=24000,
                output=True)
			for response in responses:
				if response.choices:
					# if response.choices[0].delta.content:
					# 	print(response.choices[0].delta.content, end="")
					if hasattr(response.choices[0].delta, "audio"):
						try:
							audio_string = response.choices[0].delta.audio["data"]							
							wav_bytes = base64.b64decode(audio_string)
							audio_np = np.frombuffer(wav_bytes, dtype=np.int16)
							# print(wav_bytes)
							# audio_base64_string += audio_np.tobytes()
							stream.write(audio_np.tobytes())
						except Exception as e:
							print(response.choices[0].delta.content)
			
			return audio_base64_string

		except Exception as e:
			print(f'omni model fail with error {e}')
		return b''

	def run_conversation_turn_traditional(self):
		try:
			audio_data = self.record_voice_input()
			if len(audio_data) < 3200:
				return

			user_text = self.transcribe_audio(audio_data)
			print(f'User Text: "{user_text}"')

			if not user_text or len(user_text.strip()) < 2: 
				print('User text too short, skipping')
				return 
			# 1) å…ˆé˜»å¡ç”Ÿæˆ ragï¼ˆæ‹¿åˆ°å­—ç¬¦ä¸²ç»“æœï¼‰
			dialog_rag = self.fetch_deep_research_background(user_text)
			# åå°æ‰“å°å½“å‰è½®çš„ RAG èƒŒæ™¯ï¼Œä¾¿äºè°ƒè¯•/è§‚å¯Ÿ
			print("==== DIALOG RAG START ====")
			print(dialog_rag or "ï¼ˆç©ºï¼‰")
			print("==== DIALOG RAG END ====")

			# 2) rag ç”Ÿæˆå®Œæ¯•åå†å†™å…¥ contextï¼ˆå¹¶å¼€å¯ enable_ragï¼‰
			if dialog_rag:
				self.set_rag_context(dialog_rag)

			# 3) æœ€åå†è°ƒç”¨ LLMï¼ˆæ­¤æ—¶ä¼šå¸¦ä¸Š rag_contextï¼‰
				# negotiation_turn_prompt = (
				# 	"ã€åœºæ™¯ã€‘ä½ æ­£åœ¨æ›¿ç”¨æˆ·å’Œå•†å®¶ç ä»·ã€‚\n"
				# 	"ã€å•†å®¶åˆšåˆšè¯´ã€‘\n"
				# 	f"{user_text}\n\n"
				# 	"ã€ä½ çš„ä»»åŠ¡ã€‘\n"
				# 	"- ç»™å‡ºç”¨æˆ·ä¸‹ä¸€å¥è¦å¯¹å•†å®¶è¯´çš„è¯ï¼ˆä¸­æ–‡å£è¯­ï¼Œ1-2 å¥ï¼‰ã€‚\n"
				# 	"- ä¼˜å…ˆäº‰å–â€œæ›´ä½ä»·æ ¼â€ï¼Œå¦‚æœä¸è¡Œå°±äº‰å–â€œèµ å“/åŒ…é‚®/å»¶ä¿/è¿”ç°/åˆ†æœŸä¼˜æƒ â€ç­‰ã€‚\n"
				# 	"- ç»“åˆä½ æ”¶åˆ°çš„èƒŒæ™¯çŸ¥è¯†ï¼ˆç³»ç»Ÿé‡Œçš„ context/knowledge baseï¼‰ï¼Œä¸è¦ç¼–é€ ã€‚\n"
				# )
				# assistant_text = self.get_llm_response(negotiation_turn_prompt)
			assistant_text = self.get_llm_response(user_text)
			print(f'Assistant Text: "{assistant_text}"')

			if not assistant_text:
				print('Assistant text empty, skipping')
				return 

			response_audio = self.synthesize_speech(assistant_text)
			print(f'Response Audio: {len(response_audio) if response_audio else 0} bytes')

			if not response_audio:
				print('Response audio empty, skipping')
				return 
			
			print('Playing audio...')
			self.speaker.play(response_audio) 

		except KeyboardInterrupt:
			raise
		except Exception as e:
			print(f'traditional loop fail with error {e}')

	def run_conversation_turn_omni(self):
		try:
			audio_data = self.record_voice_input()
			if len(audio_data) < 3200:
				return

			response_audio = self.process_voice_with_omni(audio_data)
			print(f'Response Audio: {len(response_audio) if response_audio else 0} bytes')

			if not response_audio:
				return 
			
			self.speaker.play(response_audio) 

		except KeyboardInterrupt:
			raise
		except Exception as e:
			print(f'omni loop fail with error {e}')

	def run_conversation_turn(self):
		if self.config.mode == 'traditional':
			self.run_conversation_turn_traditional()
		else:
			self.run_conversation_turn_omni()

	def run(self):
		self.initialize()
		self.is_running = True
		try:
			brand = input('Enter a brand to research for context (or press Enter to skip): ').strip()
			if brand:
				print(f'Running deep research for brand "{brand}"...')
				self.prime_rag_with_brand(brand)
			while self.is_running: 
				user_input = input('press enter to speak or quit for exit')

				if user_input.lower() in ['quit', 'exit', 'q']:
					break
				self.run_conversation_turn()
		except KeyboardInterrupt: 
			print('Interrupt by user')
		finally:
			self.shutdown()

	def shutdown(self):
		self.is_running=False
		self.microphone.stop()
		self.speaker.stop()

def main():
	import logging
	# Enable logging for deep research agent
	logging.basicConfig(
		level=logging.INFO,
		format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
	)
	
	if not os.environ.get("DASHSCOPE_API_KEY"):
		print("âŒ Error: DASHSCOPE_API_KEY environment variable is not set")
		sys.exit(1)
	
	# Check SearxNG availability
	searx_host = os.environ.get('SEARX_HOST', 'http://127.0.0.1:38000')
	print(f"ğŸ” Checking SearxNG at {searx_host}...")
	try:
		import requests
		response = requests.get(searx_host, timeout=2)
		if response.status_code == 200:
			print(f"âœ… SearxNG is accessible")
		else:
			print(f"âš ï¸  SearxNG returned status {response.status_code}")
	except Exception as e:
		print(f"âš ï¸  Warning: Could not connect to SearxNG at {searx_host}")
		print(f"   Error: {e}")
		print(f"   Make sure SearxNG is running: docker run -d -p 38000:8080 searxng/searxng")
	
	while True: 
		mode_choice = input('choose mode 1Traditional or 2omni with number only: ').strip()
		if mode_choice == '1':
			mode = 'traditional'
			break
		elif mode_choice == '2': 
			mode = 'omni'
			break
		else:
			print(f'invalid, please choose 1 or 2')
	
	try:
		config = VoiceAIConfig(mode=mode)
		voice_ai = RealTimeVoiceAI(config)
		voice_ai.run()
	except Exception as e: 
		import traceback
		traceback.print_exc()
		sys.exit(1)

if __name__ == "__main__":
	main()